{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcf2bdb",
   "metadata": {},
   "source": [
    "# HTTP communication & web scraping using Python built-in libraries\n",
    "\n",
    "This Jupyter notebook shows how to use **Python's standard library** to perform HTTP requests and basic web scraping. It avoids third-party libraries (like `requests` or `beautifulsoup4`) by design so you can learn the low-level building blocks.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "1. HTTP basics with `http.client` and `urllib`.\n",
    "2. Building GET and POST requests.\n",
    "3. Handling query strings and forms (`urllib.parse`).\n",
    "4. Cookies and sessions with `http.cookiejar` + `urllib.request`.\n",
    "5. Parsing HTML with `html.parser` and `xml.etree`.\n",
    "6. Downloading files (images, PDFs).\n",
    "7. Respecting `robots.txt`, headers, and rate limiting.\n",
    "8. Practical examples and mini-projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d96a2",
   "metadata": {},
   "source": [
    "## 1) `http.client` — low-level HTTP\n",
    "\n",
    "`http.client` is a low-level module for HTTP/1.1 communication. You create a connection to a host and then send requests.\n",
    "\n",
    "Example: perform a simple GET request to `example.com` and inspect the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286549f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Reason: OK\n",
      "\n",
      "Headers:\n",
      "Accept-Ranges: bytes\n",
      "Content-Type: text/html\n",
      "ETag: \"84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134\"\n",
      "Last-Modified: Mon, 13 Jan 2025 20:11:20 GMT\n",
      "Content-Length: 1256\n",
      "Cache-Control: max-age=86000\n",
      "Date: Sat, 04 Oct 2025 12:56:39 GMT\n",
      "Connection: keep-alive\n",
      "Alt-Svc: h3=\":443\"; ma=93600\n",
      "\n",
      "First 500 bytes of body:\n",
      "\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPSConnection('example.com')\n",
    "conn.request('GET', '/')\n",
    "resp = conn.getresponse()\n",
    "print('Status:', resp.status)\n",
    "print('Reason:', resp.reason)\n",
    "headers = resp.getheaders()\n",
    "print('\\nHeaders:')\n",
    "for k,v in headers:\n",
    "    print(f\"{k}: {v}\")\n",
    "body = resp.read(500)  # read first 500 bytes\n",
    "print('\\nFirst 500 bytes of body:\\n')\n",
    "print(body.decode('utf-8', errors='replace'))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61f3d7",
   "metadata": {},
   "source": [
    "### Building GET requests with query strings\n",
    "\n",
    "Use `urllib.parse` to build query strings safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a512c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full URL: https://www.example.com/search?q=python+http+client&page=1\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlencode, urlunparse\n",
    "\n",
    "params = {'q': 'python http client', 'page': 1}\n",
    "qs = urlencode(params)\n",
    "url = urlunparse(('https', 'www.example.com', '/search', '', qs, ''))\n",
    "print('Full URL:', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad78bae",
   "metadata": {},
   "source": [
    "### POST requests (form data) — `http.client` + `urllib.parse`\n",
    "\n",
    "You must encode form fields as `application/x-www-form-urlencoded`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86a2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Reason: OK\n",
      "\n",
      "Response body (truncated 800 chars):\n",
      "\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"password\": \"secret\", \n",
      "    \"username\": \"alice\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"30\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-Stdlib-Client/1.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-68e11996-488668627ca5bf9c1258b5fe\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"103.79.252.177\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "params = {'username':'alice', 'password':'secret'}\n",
    "body = urlencode(params)\n",
    "headers = {'Content-Type':'application/x-www-form-urlencoded', 'User-Agent':'Python-Stdlib-Client/1.0'}\n",
    "\n",
    "conn = http.client.HTTPSConnection('httpbin.org')\n",
    "conn.request('POST', '/post', body=body, headers=headers)\n",
    "resp = conn.getresponse()\n",
    "print('Status:', resp.status)\n",
    "print('Reason:', resp.reason)\n",
    "resp_body = resp.read().decode('utf-8')\n",
    "print('\\nResponse body (truncated 800 chars):\\n')\n",
    "print(resp_body[:800])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e5e08",
   "metadata": {},
   "source": [
    "## 2) `urllib.request` — higher-level convenience wrapper\n",
    "\n",
    "`urllib.request` wraps `http.client` and handles many details for you. It also integrates with `http.cookiejar` for cookies.\n",
    "\n",
    "Example: simple GET using `urllib.request.urlopen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65311eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "\n",
    "req = Request('https://example.com', headers={'User-Agent':'Python-urllib/3'})\n",
    "with urlopen(req) as r:\n",
    "    print('Status:', r.status)\n",
    "    print('Content-Type:', r.getheader('Content-Type'))\n",
    "    text = r.read(400).decode('utf-8', errors='replace')\n",
    "    print('\\nFirst 400 bytes:\\n')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c2ddc",
   "metadata": {},
   "source": [
    "### 3) Cookies & simple 'session' handling using `http.cookiejar` + `urllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b14749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set cookie status: 200\n",
      "\n",
      "Cookies endpoint response:\n",
      "\n",
      "{\n",
      "  \"cookies\": {\n",
      "    \"sessioncookie\": \"123456\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Stored cookies in cookie jar:\n",
      "<Cookie sessioncookie=123456 for httpbin.org/>\n"
     ]
    }
   ],
   "source": [
    "import http.cookiejar\n",
    "from urllib.request import build_opener, HTTPCookieProcessor, Request\n",
    "\n",
    "cj = http.cookiejar.CookieJar()\n",
    "opener = build_opener(HTTPCookieProcessor(cj))\n",
    "\n",
    "# Example: set a cookie via httpbin and then access /cookies\n",
    "req = Request('https://httpbin.org/cookies/set/sessioncookie/123456', headers={'User-Agent':'Python-http-cookie-example'})\n",
    "with opener.open(req) as r:\n",
    "    print('Set cookie status:', r.status)\n",
    "\n",
    "req2 = Request('https://httpbin.org/cookies', headers={'User-Agent':'Python-http-cookie-example'})\n",
    "with opener.open(req2) as r:\n",
    "    print('\\nCookies endpoint response:\\n')\n",
    "    print(r.read().decode('utf-8'))\n",
    "\n",
    "print('\\nStored cookies in cookie jar:')\n",
    "for cookie in cj:\n",
    "    print(cookie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79a7dd",
   "metadata": {},
   "source": [
    "### 4) Respect `robots.txt`\n",
    "\n",
    "Before scraping, check `robots.txt` to see allowed paths. Python has `urllib.robotparser`.\n",
    "\n",
    "Example: parse and check whether `/` is allowed for a user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caf0592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can fetch / ?  True\n",
      "Crawl-delay (may be None): <bound method RobotFileParser.crawl_delay of <urllib.robotparser.RobotFileParser object at 0x7138b81bd580>>\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('https://example.com/robots.txt')\n",
    "rp.read()\n",
    "print('Can fetch / ? ', rp.can_fetch('*', 'https://example.com/'))\n",
    "print('Crawl-delay (may be None):', getattr(rp, 'crawl_delay', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9db53",
   "metadata": {},
   "source": [
    "### 5) Parsing HTML with the built-in `html.parser` (HTMLParser)\n",
    "\n",
    "`HTMLParser` is an event-driven parser suitable for simple extraction tasks. For complicated extraction use a proper parser (e.g., BeautifulSoup), but `HTMLParser` is useful when you cannot install third-party libs.\n",
    "\n",
    "Example: extract all links (`<a href=\"...\">`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e16080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links:\n",
      "['https://example.com/about', '/local/path']\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class LinkExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.links = []\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for name, value in attrs:\n",
    "                if name == 'href':\n",
    "                    self.links.append(value)\n",
    "\n",
    "# small sample HTML\n",
    "sample = '''\n",
    "<html><body>\n",
    "<a href=\"https://example.com/about\">About</a>\n",
    "<a href=\"/local/path\">Local</a>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "le = LinkExtractor()\n",
    "le.feed(sample)\n",
    "print('Found links:')\n",
    "print(le.links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39fa2e",
   "metadata": {},
   "source": [
    "### 6) `xml.etree.ElementTree` for well-formed XML/HTML\n",
    "\n",
    "If the HTML is well-formed or you're working with XML, `xml.etree` provides XPath-like traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6373cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 One\n",
      "2 Two\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "xml = '<root><item id=\"1\">One</item><item id=\"2\">Two</item></root>'\n",
    "root = ET.fromstring(xml)\n",
    "for it in root.findall('item'):\n",
    "    print(it.get('id'), it.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c3e0f",
   "metadata": {},
   "source": [
    "### 7) Downloading files (images, PDFs) with `urllib.request`\n",
    "\n",
    "Use `urlretrieve` or `urlopen` + streaming write for large files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a1d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example (disabled run by default):\n",
      "urlretrieve(\"https://example.com/image.png\", \"image.png\")\n",
      "\n",
      "Streaming example code (save and run locally if you have internet):\n",
      "\n",
      "from urllib.request import urlopen, Request\n",
      "req = Request('https://example.com', headers={'User-Agent': 'Python-download-example'})\n",
      "with urlopen(req) as r, open('downloaded_example.html', 'wb') as out:\n",
      "    chunk = r.read(8192)\n",
      "    while chunk:\n",
      "        out.write(chunk)\n",
      "        chunk = r.read(8192)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "# WARNING: On some environments this might not allow network access. Example URL shown.\n",
    "print('Example (disabled run by default):')\n",
    "print('urlretrieve(\"https://example.com/image.png\", \"image.png\")')\n",
    "\n",
    "# Recommended pattern for streaming large files:\n",
    "code = '''\\\n",
    "from urllib.request import urlopen, Request\n",
    "req = Request('https://example.com', headers={'User-Agent': 'Python-download-example'})\n",
    "with urlopen(req) as r, open('downloaded_example.html', 'wb') as out:\n",
    "    chunk = r.read(8192)\n",
    "    while chunk:\n",
    "        out.write(chunk)\n",
    "        chunk = r.read(8192)\n",
    "'''\n",
    "print('\\nStreaming example code (save and run locally if you have internet):\\n')\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4944a",
   "metadata": {},
   "source": [
    "### 8) Politeness, headers, and rate limiting\n",
    "\n",
    "- Always send a descriptive `User-Agent` and your contact information if doing large-scale scraping.\n",
    "- Respect `robots.txt` and the site's `Crawl-delay` if present.\n",
    "- Add delays (e.g., `time.sleep(1)`) between requests.\n",
    "- Avoid hammering servers — use exponential backoff on failures.\n",
    "\n",
    "Example polite request snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b50fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would fetch iteration 1\n",
      "Would fetch iteration 2\n",
      "Would fetch iteration 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "req = Request('https://example.com', headers={'User-Agent':'pratham-example-scraper/0.1 (+mailto:you@example.com)'})\n",
    "# polite loop (do not run fast)\n",
    "for i in range(3):\n",
    "    # local demo only; in live scraping insert try/except + backoff\n",
    "    print('Would fetch iteration', i+1)\n",
    "    time.sleep(1)  # sleep between requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea2210",
   "metadata": {},
   "source": [
    "## Practical example: extracting headings from a real page\n",
    "\n",
    "Below is an example that fetches a page and extracts `<h1>`-`<h3>` headings using `HTMLParser`.\n",
    "\n",
    "(If your environment blocks outgoing HTTP, use the `sample_html` variable to test.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ecff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings found:\n",
      "h1 - Example Domain\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "class HeadingExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_heading = None\n",
    "        self.headings = []\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag in ('h1','h2','h3'):\n",
    "            self.in_heading = tag\n",
    "            self._buf = ''\n",
    "    def handle_endtag(self, tag):\n",
    "        if self.in_heading == tag:\n",
    "            self.headings.append((tag, self._buf.strip()))\n",
    "            self.in_heading = None\n",
    "    def handle_data(self, data):\n",
    "        if self.in_heading:\n",
    "            self._buf += data\n",
    "\n",
    "# Try live fetch (may fail in offline env). If fails, fallback to sample_html\n",
    "url = 'https://example.com'\n",
    "req = Request(url, headers={'User-Agent':'Python-headline-extractor/0.1'})\n",
    "try:\n",
    "    with urlopen(req) as r:\n",
    "        html = r.read().decode('utf-8', errors='replace')\n",
    "except Exception as e:\n",
    "    print('Live fetch failed or blocked in this environment:', e)\n",
    "    html = '<html><body><h1>Sample Title</h1><h2>Subtitle</h2><p>Text</p></body></html>'\n",
    "\n",
    "he = HeadingExtractor()\n",
    "he.feed(html)\n",
    "print('Headings found:')\n",
    "for tag, text in he.headings:\n",
    "    print(tag, '-', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbd5be",
   "metadata": {},
   "source": [
    "## We can furture impliment Mini-projects\n",
    "\n",
    "1. **Write a polite crawler** that given a start URL crawls up to N pages on the same domain and saves page titles. Use `urllib`, `html.parser`, `robotparser`, and `cookiejar`.\n",
    "\n",
    "2. **Image downloader**: write a script that downloads all images on a page into a folder, skipping duplicates.\n",
    "\n",
    "3. **Form submitter**: simulate a login form (to a test site like httpbin) using `http.client` POST and show how cookies persist with `cookiejar`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dc2c7-54ce-4f56-956d-413410b1841b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
